%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint,9pt]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
\usepackage{alltt}
\usepackage{xspace}
\usepackage{mathpartir}
\usepackage{stmaryrd}
\usepackage{xcolor}

\newcommand{\COMMENT}[1]{}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{POPL '15}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{2015} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Reduction for TSO}        % These are ignored unless
\preprintfooter{Reduction for TSO}   % 'preprint' option specified.

\title{Reducing TSO to SC by Reduction}
%\subtitle{TSO Simplified}

\authorinfo{Ismail Kuru\and Serdar Tasiran}
           {Koc University}
           {ikuru@ku.edu.tr/stasiran@ku.edu.tr}
\authorinfo{Ali Sezgin}
           {University of Cambridge}
           {ali.sezgin@cl.cam.ac.uk}

\maketitle

\begin{abstract}
A prominent way of analyzing programs written for relaxed memory models is to check whether it is sound, for the particular program under analysis, to assume sequential consistency (SC) which is taken to be the tractability threshold for concurrent reasoning.
The approach is based on establishing a {\em data race freedom} result, which essentially identifies for a given memory model the class of programs which cannot manifest non-SC behaviors.
The total store ordering (TSO) memory model in particular has been fully characterized: a program will be SC-like, i.e. without any non-SC behaviors, if and only if it is triangular race free.
Unfortunately, checking whether a program has a triangular race belongs, even when constrained to finite data domains, is in {\sc PSpace}.
Furthermore it is unclear what one is to do except for reasoning in full TSO semantics if the program fails to avoid those races.

In this paper, we tackle the problem of TSO program analysis possibly in the presence of triangular races.
We begin by generalizing Lipton's reduction theory, which hitherto has been restricted to SC, for TSO programs.
We define a TSO operational semantics in which every write is split into two actions, write into the buffer and its flush from the buffer.
We show how {\em moving} these actions towards each other, hence proving the atomicity of each write, is enough to prove that a TSO program is SC-like.

We then undertake the class of TSO programs with triangular races.
We define an abstraction relation among TSO programs: $P'$ abstracts $P$ if every behavior of $P$ is also a behavior of $P'$.
In essence, abstraction aims at removing conflicts between read and write actions resulting in less conflict among actions.
Less conflict in turn leads to having more actions of the desired mover type.
We illustrate the use of abstraction by transforming a sender/receiver implementation with triangular race to one without. 

We also show how our approach can be mechanized.
To this end, we present a transformation, interesting in its own right, from TSO programs to provably equivalent SC programs.
The idea is to simulate the behavior of each thread running under TSO semantics by two tightly coupled threads running under SC semantics.
By using this tranformation, we mechanically verify the atomicity, hence SC equivalence, of double checked initialization.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
\terms
program verification

\keywords
reduction, total-store ordering, sequential consistency, race freedom, safety checking 

\input{ali-commands}

\section{Introduction}
\label{sec:intro}
Both TSO and SC give the allusion that the thread-local program order is respected for memory accesses.
Unlike SC where updates are assumed to take effect instantaneously over all threads, in TSO updates are observably split into two: a locally visible update and an instantaneous remote update not necessarily simultaneous with the local update.
In formal models, this split is captured by a thread local queue in which local updates are inserted as soon as the local update occurs; entries are removed from the queue asynchronously and non-deterministically updating a single global shared memory. 
This queue is traditionally known as the store buffer.

It has been showed that it is possible to ignore the relaxed memory effects (relative to SC) if one adheres to a particular style of programming. 
Generally referred to under the heading data race freedom, these programs cannot distinguish whether they are running on an SC architecture or something weaker. 
If a relaxed memory model is proved to have this property, then proving correctness for data race free programs reduces to proving correctness under SC.
For TSO, such a theorem was proven for a class of programs, called TRF (triangular race free): a program cannot distinguish SC from TSO iff it is TRF~\cite{Owe2010}.

As for the programs that fall out of this class, and there are many more of them, the proof efforts are centered around the simplification of reasoning associated with the effect of the store buffer on the execution.
Typically, one converts a TSO program into an equivalent one which has an additional unbounded buffer per thread modeling the asynchronous remote updates. 
Methodologies consequently differ on how they simplify the reasoning. 
It could involve reducing the points where asynchronous updates can happen~\cite{AK20XX} or bounding the size of the buffer~\cite{BYZ20XX}.
So far no work has tried to cross the gap between these two classes of programs in a uniform manner.



\section{Overview}
In this section we are going to walk through several examples illustrating the main concepts of our approach.
The discussion will be necessarily kept at a semi-formal level; all relevant formal definitions are given in the following sections.
We start by explaining how one reasons about TSO programs using reduction.

\paragraph{Reduction for TSO.}
%A TRF program does not allow for an execution with the following prefix \cite{Owe2010}:
Consider a sequence of memory accesses done by two threads, $t$ and $u$, given as:
\[
{\aliwritex t y 1}\ \cdot\ {\alireadx t x 0}\ \cdot\ {\aliwritex u x 2}
\]
%Informally, this represents a sequence with an arbitrary sequence of memory operations followed by a write to $y$ by thread $t$ followed by a (possibly empty) sequence of reads done by $t$ followed by a read to $x$ followed by a write $x$ by a different thread $u$.
This represents a sequence of a write to $y$ by thread $t$, followed by a read to $x$ again by thread, followed by a write to $x$ by thread $u$.

To see why this is a {\em bad} (distinguishing) sequence for TSO, we will refer to an alternative characterization of memory accesses in TSO.
We represent read accesses as usual, but change the way write accesses are represented.
We split each write access $W$ into two distinct accesses, {\locwrite} and {\remwrite}.
Each sequence over simple memory accesses as given above will be identified with a set of sequences over the new alphabet such that i) each {\aliwrite} is replaced with {\locwrite}, ii) each {\locwrite} has a matching {\remwrite} such that the thread local order among {\locwrite}'s is preserved by their associated {\remwrite}'s.
For instance, the following sequence is one possible TSO-expansion of the SC-sequence above.
\[
{\locwritex t y 1}\ \cdot\ {\alireadx t x 0}\ \cdot\ {\locwritex u x 2}\ \cdot\ {\remwritex u x 2}\ \cdot\ {\remwritex t y 1}
\]
Observe that the ordering between the remote updates to $y$ and $x$ is the opposite of the ordering between their corresponding local updates. 
Intuitively this means that the order of updates to $x$ and $y$ as seen by $t$ ($y$ precedes $x$) and the order seen by a different thread $v$ ($x$ precedes $y$) will be different, which is impossible under SC.

We show that a similar characterization can be obtained using reduction arguments.
An execution can be generated under SC if it is possible to {\em move} all local and their matching remote updates next to each other without changing the end state of the execution.
For the example above, the question is whether it is possible to move either $\remwritex t y \dontcare$ to the left of every possible concurrent action or $\locwritex t y \dontcare$ to the right of every possible concurrent action.
The trace above itself is not problematic because both remote writes are left-movers. 
In other words, the above trace is equivalent to the following:
\[
{\locwritex t y 1}\,\cdot\,{\remwritex t y 1}\,\cdot\,{\alireadx t x 0}\,\cdot\,{\locwritex u x 2}\,\cdot\,{\remwritex u x 2}
\]
However, in the presence of other writes such a transformation can become impossible.
Consider for instance
\begin{eqnarray*}
&{\locwritex t y 1}\,\cdot\,{\alireadx t y 1}\,\cdot\,{\alireadx t x 0}\,\cdot\,{\locwritex u x 2}\,\cdot\,{\alireadx u x 2}\,\cdot\,{\alireadx u y 0}\\
&\cdot\ {\remwritex u x 2}\,\cdot\,{\remwritex t y 1}
\end{eqnarray*}
It is possible to move the local and remote writes to $x$ done by thread $u$ without changing the values read.
However, it is impossible to make the local and remote writes to $y$ done by thread $t$ without changing one of the reads: if $\remwritex t y 1$ is to the left of $\locwritex u x 2$, the read of $y$ by thread $u$ will return 1 instead of 0; if the $\locwritex t y 1$ is to the right of $\locwritex u x 2$, the read of $x$ by thread $t$ will return 2 instead of 0.

\begin{figure}[t]
\begin{alltt}
Recv()          Send(d)
 while (R);      while (!R);
 d := Data;      Data := d;
 R := \(\alitrue\);       R := \(\alifalse\);
 return d;\end{alltt}
\caption{A binary synchronous rendez-vous with two threads.}
\label{fig:rendez-vous}
\end{figure}
Consider a simple template for a binary synchronous rendez-vous message passing as given in Fig.~\ref{fig:rendez-vous}.
There are two methods, {\tt Recv} and {\tt Send}.
The receiver method {\tt Recv} spins for the flag {\tt R} until it is set to false.
Once that happens, it reads the value stored in {\tt Data}, resets {\tt R} to true and returns the value it has read.
The sender method {\tt Send} operates dually: it spins on {\tt R} until it is set to true, updates the value in {\tt Data} and resets {\tt R} to false.

Recall that a write can be treated as atomic if for any execution in which both its local and remote updates occur, one can find an equivalent execution in which they occur consecutively. 
For instance, if we want to show that the write to {\tt R} done by {\tt Recv} is atomic, we have to show that if the following is an execution
\[
A\,\cdot\,{\locwritex t R {\alitrue}}\,\cdot\,B\,\cdot\,{\remwritex t R {\alitrue}}
\]
there is a partitioning of $B$ into $B_1$ and $B_2$ such that
\[
A\,\cdot\,B_1\,\cdot\,{\locwritex t R {\alitrue}}\,\cdot\,{\remwritex t R {\alitrue}}\,\cdot\,B_2
\]
is also an execution.
One can construct such an execution because i) one can always move a local update to the right of other thread's actions, ii) there cannot be any remote updates to $R$ in the sequence of accesses represented by $B$, iii) thus it is safe to move any reads of $R$ done by a subsequent call to {\tt Send} all of which will return {\alitrue}.
Thus setting $B_1=B$ and $B_2=\varepsilon$ gives the desired execution.
Similar arguments are used to prove that all the write accesses in Fig.~\ref{fig:rendez-vous} are atomic.

\paragraph{Abstracting memory accesses.}

\paragraph{Transforming TSO into SC.}

\paragraph{Local analysis for reduction.}
Once the program is transformed, one can appeal to the reduction theorems for TSO.


Our contributions are:
\begin{itemize}
\item Formal reasoning framework for TSO programs based on reduction.
\item Systematic abstraction of a program so that the transformed has no SC-distinguishing executions and is free of assertion violations only if the original program is free of assertion violations.
\item Structural transformation of a TSO program which has the same behavior under SC.
\item Local and modular reduction framework for transformed programs.
\end{itemize}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
%\newtheorem{proof}{Proof}

%\newenvironment{proof}{\bf{Proof.\,\,}\rm}{$\hspace*{\fill}\Box$\par}

\input{formal-framework}

\input{reduction-for-tso}

\newcommand{\aliabsrulex}[1]{\textnormal{\sc\small #1}}
\newcommand{\alihavocval}{\ensuremath{\star}}

\section{Abstracting TSO programs}
\label{sec:abstracting-tso-programs}
The main problem with the existing body of work on TSO program verification is the impossibility of handling programs which do contain non SC-like runs, i.e. programs with triangular races.
Our approach so far allows us to at least alleviate some of the difficulties in reasoning by showing that certain write statements can be taken to be atomic.
In this section, we go one step further and show abstraction can be used to turn a non-SC-like TSO program into one that is SC-like.

The use of abstraction in reduction was successfully demonstrated in~\cite{EQT2009} in the context of sequentially consistent programs.
Since the soundness of the method crucially depends on the atomicity of each action, it was not clear how one can adopt those techniques to weaker memory models.
Here we address and resolve the issue of non-atomic writes for TSO.

\begin{definition}[Program Abstraction]
Let $P$ and $P'$ be two programs.
We say that $P'$ {\em abstracts} $P$, if one of the following holds:
\begin{itemize}
\item $P'$ has a failed run, or
\item $P$ does not have a failed run and for each terminated run $\alisequencex r\in\alirunsx {tso} P$, there exists a run $\alisequencex {r'}\in\alirunsx {tso} {P'}$ such that $\alimemtracex {\alisequencex r} = \alimemtracex {\alisequencex {r'}}$.
\end{itemize}
\end{definition}
Intuitively, $P'$ abstract $P$ if $P'$ contains an assertion violation or has more behaviors than $P$.
This means that if $P'$ can be proven to contain no assertion violations, then neither does $P$.
We should note that the other direction, that when $P$ does not contain an assertion violation neither should $P'$, does not hold in general.

\begin{table}
\begin{tabular}{ll}
$\aliabsrulex {InsertAssert}$ & $s \leadsto \aliatomicx {\aliassertx e; s}$\\
$\aliabsrulex {WeakAssume}$ & $\aliassumex e \leadsto \aliassumex e'$\\
& \{provided $e\Rightarrow e'$\}\\
$\aliabsrulex {ValNondet}$ & $r := \alimemx e \leadsto r := \alihavocval$\\
& $r := e \leadsto r := \alihavocval$\\ 
& $\alimemx e := r \leadsto \alimemx e := \alihavocval$\\
$\aliabsrulex {CtrlNondet}$ & 
 \begin{tabular}[t]{ll}
 $s \leadsto$ & $\aliif\ {\alihavocval}$\\
 & $\alithen\ \mathtt{\{}\aliassumex e; s'\mathtt{\}}$\\
 & $\alielse\ \mathtt{\{}\aliassumex e'; s''\mathtt{\}}$\\
 & \{provided $e\vee e'$ is tautology, and\\ 
 & $s'$ and $s''$ are abstractions of $s$\}
\end{tabular}
\end{tabular}
\caption{Substitution rules guaranteeing sound abstraction.}
\label{tab:abs-rules}
\end{table}

\paragraph{Abstraction rules.}
There are many ways to ensure that a syntactic manipulation of $P$ results in another program $P'$ abstracting the former.
In Table~\ref{tab:abs-rules}, we list several rules which are essentially individual statement replacements that provide a sound abstraction.

The first substitution rule, {\aliabsrulex {InsertAssert}}, is the insertion of an assertion to any statement. 
Recall that an assert statement either does nothing (if its predicate evaluates to true) or results in a failed run.
Both cases trivially satisfy the conditions of abstraction.

A dual rule is to weaken assumptions, the rule {\aliabsrulex {WeakAssume}}.
In this case, the assume statement with a weaker predicate will allow for more behaviors.

Another rule, {\aliabsrulex {ValNondet}}, introduces non-deterministic reads or writes.
Let $\alihavocval$ be an expression that can evaluate to any integer value.
\footnote{Essentially, with the introduction of $\alihavocval$ we are changing the evaluation operator $\alieval$ from a mapping from expressions to values to a mapping from expression to sets of values.}
Then replacing any expression with $e$ with $\alihavocval$ is another abstraction.

Finally, the rule {\aliabsrulex {CtrlNondet}}, introduces non-deterministic control flow.
The idea is to replace a statement $s$ by an if-then-else statement such that the branches are not necessarily mutually exclusive, i.e. certain states may satisfy both $e$ and $e'$ of the rule in Table~\ref{tab:abs-rules}.
This is a sound abstraction because no matter which branch is taken (and at any state at least one of these branches are enabled) whatever $s$ was doing, possibly more, in the original program will be done.  
For a detailed exposition of how abstraction in conjunction with reduction leads to a natural style of reasoning in safety proofs, the reader is referred to \cite{EQT2009}.

\paragraph{Why it works?}
Before going into examples, let us briefly discuss how abstraction leads to an SC-like program. 
Consider the template of a triangular race as given in~\cite{Owe2010} below:
\begin{eqnarray*}
&\locwritex t A 1 \cdot \alireadx t B 0 \cdot \locwritex u B 2 \cdot \remwritex u B 2\\
&\ \cdot\ \alireadx v B 2 \cdot \alireadx v A 0 \cdot \remwritex t A 1
\end{eqnarray*}
In this TSO-execution, $t$ observes the initial value for $B$, whereas $v$ observes the write to $B$ by $u$ before the write to $A$ by $t$ because the remote write action of $t$ comes after both of the reads of $v$.
In order to construct an equivalent SC-execution, both statements of $t$ should precede the statement of $u$ because the read of $B$ by $t$ returns the initial value 0.
However, for the reads of $v$ to be consistent the write to $B$ should occur before the write to $A$, which contradicts the previous requirement.

Looking more closely into the execution, we see that it is impossible to make the write to $A$ by $t$ atomic. 
Recall that for a local and remote write pair to be atomic, it must be possible to partition into two the sequence of actions that separate the two write actions such that the local write moves to the right of every action of the left partition and the remote write moves to the left of every action of the right partition.
In the given memory trace, $\locwritex t A 1$ is a right-mover but is followed by a read of $B$ which cannot move to the right of $\remwritex u B 2$.
This implies that $\remwritex t A 1$ should move to the left of every action up to $\locwritex u B 2$.
But this is impossible because it cannot move to the left of $\alireadx v A 0$.

Now consider the abstraction which removes the read of $B$ by $t$.
In other words, if the read of $B$ by $t$ is due to the statement $s$={\tt \aliregx 1:=$B$}, consider the program obtained by replacing $s$ with {\tt \aliregx 1:=\alihavocval}.
This essentially removes the read of $B$ by $t$ from the memory trace since the abstract read does not depend on any write.
This in turn means that instead of the above execution, we will have the following:
\begin{eqnarray*}
&\locwritex t A 1 \cdot \locwritex u B 2 \cdot \remwritex u B 2\\
&\ \cdot\ \alireadx v B 2 \cdot \alireadx v A 0 \cdot \remwritex t A 1
\end{eqnarray*}
which can be brought into an atomic equivalent form as:
\begin{eqnarray*}
&\locwritex u B 2 \cdot \remwritex u B 2\\
&\ \cdot\ \alireadx v B 2 \cdot \alireadx v A 0 \cdot \locwritex t A 1 \cdot \remwritex t A 1
\end{eqnarray*}
Of course, this is a simple example where the read value by $t$ is not used, so the abstraction might make sense trivially. 
We next work through several simple templates which demonstrate the power of abstraction: transforming a program into one that is SC-like which can be used to prove safety properties of interest.
 
\subsection{Send/Receive}
\label{subsec:send-receive}

\begin{figure}[h]
\begin{tabular}{p{.2\textwidth}p{.2\textwidth}}
\begin{alltt}Recv()
 \(Rdy\):=1;
 r[2]:=\(Buf\);
 if r[2]=0 then
   r[1]:=\(Flag\);
   while r[1]=0 
     r[1]:=\(Flag\);
   r[2]:=\(Buf\);\end{alltt}
&
\begin{alltt}Send(\(d\))
 \(Buf\):=\(d\);
 r[1]:=\(Rdy\);
 while r[1]=0 
   r[1]:=\(Rdy\);
 \(Flag\):=1;\end{alltt}
\end{tabular}
\caption{Sender/Receiver template with a triangular race.}
\label{fig:send-receive}
\end{figure}

Consider the code given in Fig.~\ref{fig:send-receive}, which represents a standard synchronization pattern, where a sender sets a flag after having prepared its message.
The intended operation proceeds as follows:
The sender begins by preparing the message (represented by writing $d$ into $Buf$), and then spins on $Rdy$.
After observing $Rdy$ equal to 1 it sends the message ready signal by setting $Flag$ to 1.
The receiver begins by setting $Rdy$ (initially 0) to 1 to tell the sender that it is ready to receive a message.
Before spinning on $Flag$, the receiver reads the current content of $Buf$ (0 means message not in yet).
If it observes a non-zero value denoting a valid message, it skips the entire spinning block.
Otherwise, it spins on $Flag$ and after observing it to be equal to 1, it reads the message from $Buf$.
Let us consider the set of TSO programs in which two threads $t$ and $u$ run {\tt Recv} and {\tt Send(42)} respectively.

\paragraph{Triangular race.}
This program does have a triangular race depicted by the following memory trace
\begin{eqnarray*}
& \locwritex t {Rdy} 1 \cdot\ \alireadx t {Buf} 0\cdot\ \locwritex u {Buf} d \cdot\ \remwritex u {Buf} d\\
&\cdot\ \alireadx u {Rdy} 0\cdot\ \remwritex t {Rdy} 1
\end{eqnarray*}
The first three actions, the local write to $Rdy$ and read of $Buf$ by $t$ followed by a write to $Buf$ by $u$ gives the triangular race.
The remaining part of the execution shows how the race can be extended into a non SC-like run. 

\paragraph{Removing the triangular race.}
Following the discussion about the general case regarding triangular races, one might be tempted to abstract the read of $Buf$ by $t$ altogether.
However, that would make the rest of the code behave incorrectly since a non-zero value for $Buf$ means a valid message which need not be equal to what $u$ is about to write, $d$.

Let us instead consider the following abstraction for the statement {\tt $\aliregx 2$:=$Buf$}:
\begin{eqnarray*}
&&\aliif\ {\alihavocval}\\
&&\alithen\ \aliatomicx {\aliassumex {Buf=0};\ \aliregx 2:=Buf;}\\
&&\alielse\ \aliatomicx {\aliassumex {Buf\neq0};\ \aliregx 2:=\alihavocval;}
\end{eqnarray*}
This replacement is an instance of {\sc\small CtrlNondet} and thus leads to a sound abstraction.
We claim that in the new abstract program $P'$ the write to $Rdy$ by {\tt Recv} is atomic.
%In order to prove this claim we have to show that the memory trace of any run of $P'$ has an equivalent SC-execution.
Let $\alisequencex r$ be a run in $\alirunsx {tso} {P'}$ whose memory trace is of the form
\[
\alpha\cdot\ \locwritex t {Rdy} 1\cdot\ \tau\cdot\ \remwritex t {Rdy} 1\cdot\ \beta
\]
where $\alpha$, $\tau$ and $\beta$ are sequences of memory actions.
We have to show that there are sequences of actions $\tau_1$ and $\tau_2$ such that $\tau=\tau_1\tau_2$, $\locwritex t {Rdy} 1$ moves to the right of every action in $\tau_1$, and $\remwritex t {Rdy} 1$ moves to the left of every action in $\tau_2$. 
We set $\tau_1=\tau$ and $\tau_2=\varepsilon$ and show that the following memory trace belongs to a run of $P'$.
\[
\alpha\cdot\ \tau\cdot\ \locwritex t {Rdy} 1\cdot\ \remwritex t {Rdy} 1\cdot\ \beta
\]

First, observe that $\tau_1$ cannot contain the write to $Flag$ by $u$ because that action only happens when $u$ ends its spinning by reading 1 from $Rdy$ which can only happen in $\beta$, i.e. after the remote write action $\remwritex t {Rdy} 1$.
This in turn means that the farthest $t$ can go in $\tau$ is the reading of $Flag$.
Any read action is a right (and left) mover with respect to another action not writing to the location read.
Thus, in case $\tau_1$ contains read(s) of $Flag$ they can all move to right until $\remwritex t {Rdy} 1$.

Since a local write action is always a right mover, we are left with the read of $Buf$ by $t$.
If the remote write action $\remwritex u {Buf} d$ is not in $\tau$, then we are done.
Assume that \remwritex u {Buf} d happens after some prefix $\tau_p$ of $\tau$, i.e.
\[
\alpha\cdot\ \locwritex t {Rdy} 1\cdot\ \tau_p\cdot\ \remwritex u {Buf} d\cdot\ \tau_q\cdot\ \remwritex t {Rdy} 1\cdot\ \beta
\]
If the read of $Buf$ by $t$ happens in $\tau_q$, we are done.
So assume that the read of $Buf$ happens in $\tau_p$.
Since it happens before the remote write action, the state at which the abstract read statement occurs must have $Buf=0$.
According to the abstraction, this means that the {\tt then} branch must have been taken, which sets $\aliregx 2$ to 0.
If the read action moves to the right of $\remwritex u {Buf} d$, then because $Buf\neq 0$ holds after the remote write, the {\tt else} branch will be taken instead.
That in turn implies that $\aliregx 2$ can be assigned any value, including 0. 
So, the sequence
\[
\alpha\cdot\ \cdot\ \tau_p'\cdot\ \remwritex u {Buf} d\cdot\ \tau_q'\cdot\ \locwritex t {Rdy} 1\cdot\ \remwritex t {Rdy} 1\cdot\ \beta
\]
where $\tau_p'$ does not contain any action by $t$ is also a memory trace of $P'$.
Since $\tau$ was taken to be arbitrary, we conclude that the write to $Rdy$ by $t$ is atomic.

\paragraph{Execution context.}
Observe that our argument also establishes the remote write action to $Buf$ as a left mover. 
However, there is a fundamental difference between the two arguments.
Since we proved the atomicity of the write to $Rdy$ by showing that all actions of $t$ until the remote write action to $Rdy$ occurs are right-movers, our result does not depend on the client's state, in particular the store buffer state. 
On the other hand, because our argument for the atomicity of the write to $Buf$ is based on showing that its remote write action is a left-mover, the result depends on the client's store buffer state. 
For instance, if both threads' buffers contained remote write actions to the same location before calling {\tt Recv} and {\tt Send} then we will not be able to move the remote write to $Buf$ next to its matching local write.
In other words, if one wants to have atomicity of these writes in any execution context, {\tt Recv} must end with a fence whereas {\tt Send} must begin with a fence.


\section{Mechanical Verification of Write Atomicity}
\label{sec:mechanical-verification}
In this section, we show how the atomicity of a write can be mechanically checked.
We begin by describing a transformation which maps TSO programs to {\em equivalent} SC programs.
The asynchronous nature of the buffered updates is captured by a splitting of each thread into two threads, one representing the local operations, the other representing the remote operations.
The queue semantics of each store buffer is captured by program order enforced by SC.
The results we have presented thus far are used to verify the atomicity of any statement  in the transformed program under SC semantics.
We illustrate our methodology by verifying the atomicity of writes in double checked initialization. 
%In particular, we show how mover types of certain statements can be used to conclude that a program cannot have any TSO distinguishing behavior.
%Finally, we define an abstraction relation among TSO programs.
%This abstraction forms the pillar of a reasoning methodology which enables one to start with a program with TSO distinguishing behavior and to end with a more abstract program which is SC-like.


\begin{figure*}
\begin{tabular}{p{.35\textwidth}p{.3\textwidth}p{.34\textwidth}}
\begin{alltt}\(RdL(e,r,l)\) \{  
 st:=StExec[\(l\)][tid]+1;
 ver:=AdrVer[\(e\)][tid];
 if ver>AdrVer[\(e\)][tido]
 then \{\(r\):=WrVal[e][ver][tid]; \}
 else \{\(r\):=mem[\(e\)];
 RdVal[l][st][tid]:=r;
 StExec[\(l\)][tid]:=st;
\}

\(RdR(r,l)\) \{
 st:=StExec[\(l\)][tid]+1;
 \(\aliassume\) st<=StExec[\(l\)][tido];
 r := RdVal[\(l\)][st][tido];
 StExec[\(l\)][tid]:=st;
\}
\end{alltt} &

\begin{alltt}\(WrL(e,r,l)\) \{
 st:=StExec[\(l\)][tid]+1;
 ver:=AdrVer[\(e\)][tid]+1;
 WrVal[\(e\)][ver][tid]:=r;
 AdrVer[\(e\)][tid]:=ver;
 StExec[\(l\)][tid]:=st;
\}

\(WrR(e,l)\) \{
 st:=StExec[\(l\)][tid]+1;
 \(\aliassume\) st<=StExec[\(l\)][tido];
 ver:=AdrVer[\(e\)][tid]+1;
 mem[e]:=WrVal[\(e\)][st][tido];
 AdrVer[\(e\)][tid]:=ver;
 StExec[\(l\)][tid]:=st;
\}\end{alltt} & 

\begin{alltt}
\(FenceL\) \{
 \(\aliassume \forall e.\) 
   AdrVer[\(e\)][tid]
        ==
   AdrVer[\(e\)][tido];
\}

\(FenceR\) \{
 \(\aliskip\);
\}\end{alltt}
\end{tabular}
\caption{The TSO to SC transformation macros.}
\label{fig:transformation-macros}
\end{figure*}

\subsection{Program Transformation}
\label{subsec:program-transformation}
Let $P=(\{M_1,\ldots,M_n\},\alilabel)$ be a labeled program.
For notational convenience, we let $l_s$ to denote the label $l$ of $s$, i.e. $\alilabelx s=l$.
The {\em split transformation} of $P$ is another program $\alisplitprogx P=(\{M^{loc}_1,M^{rem}_1,\ldots,M^{loc}_n,M^{rem}_n\},\alilabel')$, whose components are explained below.

In order to explicitly split the local and remote writes, we introduce several arrays:
\begin{itemize}
\item {\tt AdrVer[$i$][$t$]} holds the version number of updates to $\alimemx i$ by thread $t$.
\item {\tt WrVal[$i$][$v$][$t$]} holds the value written to $\alimemx i$ with version number $v$ by thread $t$.
\item {\tt RdVal[$i$][$v$][$t$]} holds the value read from $\alimemx i$ by $t$ with version number $v$.
\item {\tt StExec[$l$][$t$]} holds the number of times the statement with label $l$ is executed by $t$.
%\item {\tt Other[$t$]} holds the thread identifier matching with $t$ (see below).
\end{itemize}

Let $s$ be a statement in $M_i$.
The {\em local transformation} and the {\em remote transformation} of $s$, written as $\aliloctransx s$ and $\aliremtransx s$ respectively, are given below:
\begin{eqnarray*}
 \aliloctransx s  & \stackrel{def}{=} &
  \begin{cases}
   \aliatomicx {RdL(e,r,l_s)} & , s=r:=\alimemx e\\
   \aliatomicx {WrL(e,r,l_s)} & , s=\alimemx e:= r\\
   \aliatomicx {FenceL} & , s=\alifence\\
   s & , \textnormal{otherwise}
  \end{cases}\\
 \aliremtransx s & \stackrel{def}{=} &
  \begin{cases}
   \aliatomicx {RdR(e,r,l_s)} & , s=r:=\alimemx e\\
   \aliatomicx {WrR(e,r,l_s)} & , s=\alimemx e:= r\\
   \aliatomicx {FenceR} & , s=\alifence\\
   s & , \textnormal{otherwise}
  \end{cases}
\end{eqnarray*}
By convention, we let $\aliloctransx \varepsilon=\aliremtransx \varepsilon=\varepsilon$.
The definitions of the macros $RdL$, $RdR$, etc. are given in Fig.~\ref{fig:transformation-macros}.
Both of these transformations are extended to code blocks respecting the program order.
Formally, if $C=s;C'$ is a code block, $\aliloctransx C$ and $\aliremtransx C$ are given by $\aliloctransx s; \aliloctransx {C'}$ and $\aliremtransx s; \aliremtransx {C'}$, respectively.
This way we identify each method $M_i=\{C_i\}$ with two methods $M^{loc}_i=\{\aliloctransx {C_i}\}$ and $M^{rem}_i=\{\aliremtransx {C_i}\}$ by applying $\aliloctrans$ and $\aliremtrans$ to each statement in $M_i$.


Intuitively, this transformation separates local writes from remote writes by way of adding a new auxiliary thread.
The execution of method $m_i$ by some thread $t$ will be simulated by the execution of the methods $\tau_l(m_i)$ and $\tau_r(m_i)$ by threads $t$ and $t'$, respectively.
All statement macros keep track of how many times a statement with label $l$ is executed, the value in {\tt StExec[$l$][tid]}.

\paragraph{Read macros.}
A read statement $s=r:=\alimemx e$ by $t$ is simulated by $t$ executing $\aliatomicx{RdL(e,r,l_s)}$ and $t'$ executing $\aliatomicx{RdR(r,l_s)}$.
The local read starts by reading the execution count for $s$ and update count to {\alimemx e} by $t$.
It then checks whether it should read from the {\em buffer} or from the {\em memory}.
If the number of updates done by $t$ (local write actions) and the number of updates done by $t'$ (remote write actions) are equal, then memory is read; otherwise, the latest local write to $\alimemx e$ is read, the value in {\tt WrVal[$e$][ver][$t$]}.
It registers the value it read with the current execution count into {\tt RdVal[$l_s$][st][$t$]}.
It ends with updating the statement execution count.

A remote read starts by reading the execution count of $s$ by $t'$.
It then makes sure that the current statement is enabled by checking whether the remote execution count of $s$ is at most equal to local execution count of the same statement (by $t$).
If the assumption is satisfied, then the value as read by the local thread, the value in {\tt RdVal[$l_s$][st][$t$]}, is assigned to $r$.
It ends with updating the statement execution count.

Note that, since all macros are within {\aliatomic} blocks, either the whole macro executes or none of it. 
This implies that the remote read macro can only execute after its associated local read macro has executed.
An alternative transformation would be to make the local and remote read operations tightly coupled: each local read macro is immediately followed by its associated remote read macro.
This would have the advantage of avoiding the assume statement for read macros.
For the sake of uniformity, though, since such assume statements are essential in capturing the asynchronous nature of remote writes, we opted for this encoding.

\paragraph{Write macros.}
A write statement $s=\alimemx e:=r$ by $t$ is simulated by $t$ executing $\aliatomicx{WrL(e,r,l_s)}$ and $t'$ executing $\aliatomicx{WrR(e,l_s)}$.
The local write starts by reading the current statement count $c$ for $s$ and update count $v$ to {\alimemx e} by $t$.
It then registers the current update value into {\tt WrVal[$e$][$v$][$t$]}.
It ends with updating the update and statement execution counts.

The remote write starts by reading the current statement count $c$ for $s$ by $t'$.
It then ensures that the current macro is enabled by checking that the number of times the local counterpart of the statement is executed is at least as big as $c$.
If that is the case, the remote update count for $e$ is updated and the value written by the local counterpart, the value in {\tt WrVal[$e$][$c$][$t$]}, is written into location $\alimemx e$.
It ends with updating the update and statement execution counts done by $t'$.

\paragraph{Fence macros.}
A fence statement $s=\alifence$ by $t$ is simulated by $t$ executing $\aliatomicx{FenceL}$ and $t'$ executing $\aliatomicx{FenceR}$.
Recall that $t$ can execute $\alifence$ only when its store buffer is empty, i.e. all the remote write actions corresponding to the local write actions by $t$ have occurred.
In order to ensure this, local fence blocks until for every location $e$, the local update count is equal to the remote update count (note that the remote update count can never exceed the local update count). 
The remote fence is simply set to be no-op, i.e. \aliskip.

It might seem counter-intuitive to give the remote thread the capability of moving beyond a fence statement when the local has not executed yet, but as we shall see as far as the soundness of the transformation is concerned, only that the remote write actions are properly simulated (correct ordering with correct values) is enough.

We set $T'$ be the set of {\em primed} thread identifiers derived from $T$, such that $t'\in T'$ iff $t\in T$.
For notational convenience, we let $(t')'=t$.
In order to avoid inessential technicality, we modify the operational semantics.
We update the rule {\sc\small Init} to account for the local-remote pairing threads.
\begin{mathpar}
\inferrule*
{Rl=\textnormal{\sc\small Init'} \\ M_i\in P \\ M_i= n(a)\ \{C\} \\ \alicontrolx t =\varepsilon}
{\alivaluation[(t,\alireg_{in})\mapsto a] \\ \alicontrol[t\mapsto \alifirstx {\tau_l(C)}, t'\mapsto \alifirstx {\tau_r(C)}]}
\end{mathpar}  
We change the definition of {\alienabledx t} as follows:
\[
\alienabledx t \stackrel{def}{=} (\aliatomiclock=-1) \ \vee\ (\aliatomiclock=t)\ \vee\ (\aliatomiclock=t')
\]
Finally, we introduce a new keyword {\tt tido} such that for any valuation $\alivaluation$ and thread $t$, we have $\alivaluation\alievalx {\mathtt{tido}} t=t'$.

\newcommand{\locseqequiv}{\ensuremath{\sim_l}}

\subsection{Soundness of Transformation}
\label{subsec:soundness}
In this subsection, we prove that the program transformation defined above is sound.
That is, $P$ running under TSO semantics is equivalent to $\alisplitprogx P$ running under SC semantics.
To that end, we construct a bijection between the TSO-compliant runs of $P$ and the $\locseqequiv$-quotient of SC-compliant runs of $\alisplitprogx P$.
We begin by defining the equivalence relation $\locseqequiv$, pick a representative from each $\locseqequiv$ class, and establish the bijective relation.

Let us begin with an important observation about the orderings of local and remote transitions.
\begin{lemma}\label{lem:transformation-bijection}
Let $P$ be a program and $\alisplitprogx P$ be its transformation.
Then, in any SC-compliant run of $\alisplitprogx P$, there is a bijection $\mu$ between all local and remote transitions such that the assignments to registers are identical.
Furthermore, if $x_l$ is a local read or write transition and $x_r=\mu(x_l)$ then $x_l$ occurs before $x_r$.
\end{lemma}
\begin{proof}[Proof (Sketch)]
Observe that the only time the two threads can diverge is when they read from memory, i.e. a {\sc\small Rd} transition.
But in that case, the value read by the local thread is registered and the remote read gets that value.
The second part follows from the use of {\aliassume} statements in the remote actions and the bijection can be established by matching each local transition with a remote transition such that they both observe the same (unique) value for {\tt st}.
\end{proof}

Let $\alisequencex r$ and $\alisequencex {r'}$ be permutationally equivalent SC-compliant runs of $\alisplitprogx P$.
They are {\em locally equivalent}, $\alisequencex {r}\locseqequiv \alisequencex {r'}$, if the projections of their traces to local actions (executed by unprimed thread identifiers) and remote write actions ({\sc\small Wr}-transitions executed by primed identifiers) are identical.
Formally, $\alisequencex r\locseqequiv \alisequencex {r'}$ iff 
\[
\alitracex {\alisequencex r}\downarrow_{\{-,T:-\}\cup\{{\sc\small Wr},T':-\}} = \alitracex {\alisequencex {r'}}\downarrow_{\{-,T:-\}\cup\{{\sc\small Wr},T':-\}}
\]
where $\downarrow_{\{R,A:S\}}$ is the projection operator which removes all transitions $r,t:s$ such that $r\notin R$ or $t\notin T$ or $s\notin S$ and $-$ stands for empty set.

The run $\alisequencex {r}$ is called {\em canonical} if for all $i<|\alisequencex {r}|$, $t\in T$ with $\alitracex {\alisequencex {r}}[i]=R,t:\aliloctransx s$, the following conditions are satisfied:
\begin{itemize}
\item if $R\notin \{\textnormal{\sc\small Wr,Fnc}\}$, then $\alitracex {\alisequencex {r}}[i+1]=R,t':\aliremtransx s$.
\item if $R=\textnormal{\sc\small Fnc}$, then $\alitracex {\alisequencex {r}}[i+1]=\textnormal{\sc\small Skp},t':\aliremtransx s$.
\end{itemize}
Informally, a run is canonical if all local actions (transitions executed by unprimed threads) are immediately followed by their matching remote actions, except for write actions.

\begin{lemma}\label{lem:canonical}
Let $\alisequencex {r}$ be a TSO-compliant run of $P$.
Then there exists a unique canonical run in $[\alisequencex {r}]_{\locseqequiv}$.
\end{lemma}
\begin{proof}[Proof (Sketch)]
A remote transition (transitions of the form $R,t':\aliremtransx s$) either is completely defined by its matching local transition (when $R=\textnormal{\sc\small Rd}$) or by its local state (when $R\notin\{\textnormal{\sc\small Wr},\textnormal{\sc\small Rd}\}$).
This means that all memory independent transitions $R,t':\aliremtransx s$ are both-movers relative to all transitions but those done by $t'$.
All read transitions $\textnormal{\sc\small Rd},t':\aliremtransx s$ are left-movers and by construction cannot come before their matching local actions.
This implies that all transitions can be rearranged to obtain a canonical run within the same equivalence class.
Uniqueness comes from the fact that within the equivalence class the relative ordering among local actions and remote writes is preserved which means that in the canonical sequence each remote action has a well-defined position.
\end{proof}
We conclude by stating the main result of this section.

\begin{theorem}\label{thm:soundness}[Soundness of Transformation]
Let $P$ be a program.
There exists a bijection between all SC-compliant canonical runs of $\alisplitprogx P$ and all TSO-compliant runs of $P$.
\end{theorem}
\begin{proof}[Proof (Sketch)]
Let $\alisequencex r$ be a TSO-compliant run of $P$.
We construct an SC-compliant run $\alisequencex {r'}$ of $\alisplitprogx P$.
Every {\sc\small Init} transition of $\alisequencex r$ is replaced with an {\sc\small Init'} transition.
Every $R,t:s$ with $R\in \{\textnormal{\sc\small RdM},\textnormal{\sc\small RdB}\}$ is replaced with $\textnormal{Rd},t:\aliloctransx s$ followed by $\textnormal{Rd},t:\aliremtransx s$.
Every $\textnormal{\sc\small WrB},t:s$ is replaced with $\textnormal{\sc\small Wr},t:\aliloctransx s$.
Every $\textnormal{\sc\small WrM},t:s$ is replaced with $\textnormal{\sc\small Wr},t':\aliremtransx s$.
Finally, all other transitions $R,t:s$ are replaced with $R,t:\aliloctransx s$ followed by $R,t':\aliremtransx s$.
That such a run exists follows from Lemma~\ref{lem:transformation-bijection}.
For the other direction, we apply the same replacement rules in inverse form to obtain a TSO-compliant run of $P$ from an arbitrary SC-compliant canonical run of $\alisplitprogx P$.
\end{proof}
\begin{corollary}
Let $P$ be a program.
There exists a surjection from the SC-compliant runs of $\alisplitprogx P$ to the TSO-compliant runs of $P$.
\end{corollary}
\begin{proof}
By Lem.~\ref{lem:canonical} any SC-compliant run has an $\locseqequiv$ canonical run which can then be transformed into a TSO-compliant run using the construction of the previous proof in opposite direction.
\end{proof}





\subsection{Example - Double Checked Initialization}
\label{subsec:example-double-check}

\begin{figure}[ht]
\begin{alltt}DoubleCheckInit(Obj) 
\{
 r:=\(\alimem\)[Obj];
 \(\aliif\) r=0 \(\alithen\) 
 \{
  \(\aliatomic\) \{ \(\aliassume\) {\(\alimem\)[l]=0}; \(\alimem\)[l]:=tid; \(\alifence\); \}
  r:=\(\alimem\)[Obj];
  \(\aliif\) r=0 \(\alithen\) \{ r:=NewIdx(); \(\alimem\)[Obj]:=r; \}
  \(\alimem\)[l]:=0;
 \}
\}\end{alltt}
\caption{The code for double check initialization. The parameter {\tt a} holds the address for {\tt Obj}.}
\label{fig:double-check}
\end{figure}

Consider the code given in Fig.~\ref{fig:double-check}, which is derived from double checked initialization.
The objective of the procedure is to ensure that a shared object $Obj$ is initialized exactly once.
In order to simplify presentation, we assume that $Obj$ fits in a single memory slot, addressed by {\alimemx {\tt Obj}}.

The procedure starts by checking whether the object of interest $Obj$ has already been initialized.
If $Obj$ has indeed been initialized, the procedure terminates.
Otherwise, the procedure switches to the initialization phase.
This phase starts by acquiring a (global) lock which protects the initialization operation on the object.
If the lock is available, which is when {\alimemx{l}} is equal to 0, it is acquired by setting {\alimemx{l}} to the thread identifier of the current thread, {\tt tid}, and flushing the contents of the buffer ensuring the global visibility of the lock being acquired.
Interestingly enough, this is the only place in the code where a fence is used.

After successfully acquiring the lock, the current state of $Obj$ is checked again.
This is necessary to ensure that no other (concurrent thread) has initialized $Obj$ in the meantime.
If $Obj$ is initialized by some other thread, the lock is released by resetting {\alimemx{l}} back to 0 and the procedure terminates.
Otherwise, a new memory slot is prepared and $Obj$ is assigned to this new slot, which is followed by the release of the lock and the termination of the procedure.

I think {\tt xcolor} messes things up.

\section{Discussion}
It is not fair to claim that the work presented in this paper exists in a vacuum disconnected with what exists and has no openings to new related domains.
We end the paper by arguing to the contrary on both accounts.

\paragraph{Mover Analysis in SC.}
This is perhaps not surprising: There is an intimate connection between atomicity proofs for programs under SC semantics and the write atomicity proofs we present here.
We should first identify the subtle difference between the two usages of {\em atomicity}, which regrettably has acquired an ambiguous status.
A program is atomic if every interleaving execution of its methods is equivalent to a sequential execution in which methods execute in isolation. 
This definition of atomicity can be applied to code blocks, which is what reduction theory is essentially used for under SC semantics.
It is typical practice to prove that a particular write or read is a mover to increase the size of an atomic block, thereby decreasing the number of possible interleavings.

One natural question to ask is whether we can do what is being done for SC atomicity proofs to show that a program $P$ is TRF, without even using the split transformation.
That is, can we use the mover type of a write statement in $P$ under SC semantics to deduce that it is atomic under TSO semantics?
The answer to this question is in general negative.
Consider the occurrences of the local and remote write actions of a write statement $s$ under TSO semantics:
\[
\alpha\ \cdot\ \locwritex t x v\ \cdot\ \gamma\ \cdot\ \remwritex t x v\ \cdot\ \beta
\]
The way the tool {\sc qed} determines the mover type of an action is to check whether that action moves to the left (right) of every other concurrent action.
The use of assertions aims at capturing refining the set of potential concurrent actions, i.e. an over-approximation of the actions that might occur in $\gamma$.
This leads us to a couple of {\em backward compatibility} results, i.e. porting mover types under SC to TSO.
\begin{lemma}
Let $P$ be a TSO program and $s$ be a statement in it.
Assume that $s$ is proved to be left-mover in {\sc qed} under SC semantics (uses the same $P$ replacing all {\alifence} statements with {\aliskip}). 
\begin{itemize}
\item If $s$ is a simple write statement, $s$ is atomic under TSO semantics.
\item if $s$ is of the form $\aliatomicx {\aliassertx {\varphi}; w}$, $\varphi$ is a predicate which can be falsified by write statements none of which is ever buffered, then $w$ is atomic under TSO semantics.
\end{itemize}
\end{lemma}
\begin{proof}[Proof (Sketch)]
If $s$ is a simple write statement, then it cannot move to the left of another action by introducing a failure.
This implies that it does not conflict with any other statement in the program.
The additional statements in TSO semantics do not introduce additional conflicts (a write under SC and its remote action under TSO have the same interference relative to concurrent actions).

In the second case, the assertion helps the checker to narrow down the set of concurrent actions as explained above.
Since the remote write 
\end{proof}

\paragraph{Other Relaxed Memory Models.}
What makes a memory model relaxed is



{\sc Ali stopped here.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Ali stopped here  %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Conclusion}


\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}

\end{document}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Comments after this point %%%%%%%%%%%%
%%%%%%%%%%%%%%% Comments after this point %%%%%%%%%%%%
%%%%%%%%%%%%%%% Comments after this point %%%%%%%%%%%%
%%%%%%%%%%%%%%% Comments after this point %%%%%%%%%%%%
%%%%%%%%%%%%%%% Comments after this point %%%%%%%%%%%%
%%%%%%%%%%%%%%% Comments after this point %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%











\COMMENT{
\paragraph{Triangular race.}
This code has a triangular race due to the first read of $Obj$, but before getting into that let us first discuss why the read of $Obj$ within the lock protected region does not lead to a triangular race.
Let us use $R_1$ and $R_2$ to denote the first and second read statements of $\alimemx {\tt Obj}$. 
Let $P$ be the program containing only {\tt DoubleCheckInit}.

Let $\alisequencex r$ be a TSO-run of the form 
\[
\alisequencex r = q_0\tau_1\cdot\ q\xrightarrow{\textnormal{\sc\small Rd},t:R_2}q'\ \cdot\tau_2q_n
\]
For $\alisequencex r$ to contain a triangular race, one condition requires that $t$ have done a write on some variable other than that is read by $R_2$, i.e. a local write statement to a location different from {\tt Obj} in $\tau_1$.
Such a write, the one that updates {\alimemx{l}} in order to acquire the lock, exists.
The second condition requires that another thread write concurrently to {\alimemx{\tt Obj}} as the first transition in $\tau_2$.
But clearly such a concurrent write cannot occur because all writes to {\alimemx{\tt Obj}} are protected by the lock and at the beginning of $\tau_2$, the thread $t$ holds the lock.
Thus, had it not been for the first read $R_1$, this procedure would have been triangular race-free and SC-like.

Now let us repeat the same consideration for $R_2$.
Let the TSO-run $\alisequencex r$ be in the form
\[
\alisequencex r = q_0\tau_1\cdot\ q\xrightarrow{\textnormal{\sc\small Wr},t:W_l}\cdot\ \tau_2\ \cdot q'\xrightarrow{}
\]

}

%%%%%%%%%%%%
%%%the following should go into the QED section where we describe how SC based analysis tools 
%%%can be used to decide the mover types of remote or local write actions.
%%%%%%%%%%%%%
\COMMENT{
We begin by describing a transformation which maps TSO programs to {\em equivalent} SC programs.
We should emphasize that our construction does not employ explicit arrays, but rather implicitly models the locally ordered buffered writes by simulating each thread with two threads.
The SC program enables us to apply reduction directly.
In particular, we show how mover types of certain statements can be used to conclude that a program cannot have any TSO distinguishing behavior.
Finally, we define an abstraction relation among TSO programs.
This abstraction forms the pillar of a reasoning methodology which enables one to start with a program with TSO distinguishing behavior and to end with a more abstract program which is SC-like.


\begin{figure*}
\begin{tabular}{p{.4\textwidth}p{.3\textwidth}p{.3\textwidth}}
\begin{alltt}\(ReadL(e,r,l\sb{s})\) \{\COMMENT{ \(\aliatomic\) \{   }                                 
 if (UCntL[\(e\)][tid]>UCntR[\(e\)][tid])       
  \{\(r\) := \(e\sp{loc}\);\} \{\(r\) := \(\alimemx{e}\);\}                      
 ReadVal[\(l\sp{loc}\sb{s}\)][tid] := \(r\);               
 \(l\sp{loc}\sb{s}\)[tid]++;
\}

\(ReadR(e,r,l\sb{s})\) \{
 \(\aliassume\) \(l\sp{rem}\sb{s}\)[tid] < \(l\sp{loc}\sb{s}\)[tid];
 r := ReadVal[\(l\sp{rem}\sb{s}\)][tid];
 \(l\sp{rem}\sb{s}\)[tid]++;
\}
\end{alltt} &

\begin{alltt}\(WriteL(e,r,l\sb{s})\) \{
 \(e\sp{loc}\) := \(e\);
 \(l\sp{loc}\sb{s}\)[tid]++;
 UCntL[\(e\)][tid]++;

\(WriteR(e,s,l\sb{s})\) \{
 \(\aliassume\) \(l\sp{rem}\sb{s}\)[tid] < \(l\sp{loc}\sb{s}\)[tid];
 \(\alimemx{e}\) := WriteVal[\(l\sp{loc}\sb{s}\)][tid];
 \(l\sp{rem}\sb{s}\)[tid]++;
 UCntR[\(e\)][tid]++;
\}\end{alltt} & 

\begin{alltt}
\(FenceL\) \{
 \(\aliassume \forall e.\) 
   UCntL[\(e\)][tid]
        ==
   UCntR[\(e\)][tid];
\}

\(FenceR\) \{
 \(\aliskip\);
\}\end{alltt}
\end{tabular}
\caption{The TSO to SC transformation macros.}
\label{fig:transformation-macros}
\end{figure*}

\subsection{Program transformation.}
\label{subsec:program-transformation}
Let $P=(\{M_1,\ldots,M_n\},\alilabel)$ be a labeled program.
For notational convenience, we let $l_s$ to denote the label $l$ of $s$, i.e. $\alilabelx s=l$.
The {\em split transformation} of $P$ is another program $\alisplitprogx P=(\{M^{loc}_1,M^{rem}_1,\ldots,M^{loc}_n,M^{rem}_n\},\alilabel')$, whose components are explained below.

Let $s$ be a statement in $M_i$.
The {\em local transformation} and the {\em remote transformation} of $s$, written as $\aliloctransx s$ and $\aliremtransx s$ respectively, are given below:
\begin{eqnarray*}
 \aliloctransx s  & \stackrel{def}{=} &
  \begin{cases}
   \aliatomicx {ReadL(e,r,l_s)} & , s=r:=\alimemx e\\
   \aliatomicx {WriteL(e,r,l_s)} & , s=\alimemx e:= r\\
   \aliatomicx {FenceL} & , s=\alifence\\
   s & , \textnormal{otherwise}
  \end{cases}\\
 \aliremtransx s & \stackrel{def}{=} &
  \begin{cases}
   \aliatomicx {ReadR(e,r,l_s)} & , s=r:=\alimemx e\\
   \aliatomicx {WriteR(e,r,l_s)} & , s=\alimemx e:= r\\
   \aliatomicx {FenceR} & , s=\alifence\\
   s & , \textnormal{otherwise}
  \end{cases}
\end{eqnarray*}
By convention, we let $\aliloctransx \varepsilon=\aliremtransx \varepsilon=\varepsilon$.

These transformations are used to implicitly represent the store buffer.
The local transformation of a read statement $r\mathtt{:=}\alimemx e$ checks whether the latest update to $\alimemx e$ by this thread is still in its buffer.
If so, the value to be read from the buffer is mimicked by reading the new auxiliary variable $e^{loc}$ as seen by this thread.
Otherwise, the value is read from the memory, i.e. the contents of $\alimemx e$.

Both of these transformations are extended to code blocks respecting the program order.
Formally, if $C=s;C'$ is a code block, $\aliloctransx C$ and $\aliremtransx C$ are given by $\aliloctransx s; \aliloctransx {C'}$ and $\aliremtransx s; \aliremtransx {C'}$, respectively.
This way we identify each method $M_i=\{C_i\}$ with two methods $M^{loc}_i=\{\aliloctransx {C_i}\}$ and $M^{rem}_i=\{\aliremtransx {C_i}\}$ by applying $\aliloctrans$ and $\aliremtrans$ to each statement in $M_i$.
}
\COMMENT{
We call an SC run of $\alisplitprogx P$ {\em well-formed} if 
\begin{itemize}
\item it admits a partitioning $\aliloctransx T$ and $\aliremtransx T$ of $T$ such that $t\in\aliloctransx T$ iff 
\item there is a bijection $\mu$ among the set of thread and method pairs $(t,m)$ such that if $\mu(t,m)=(t',m')$, then $t$ is executing $m$ , $t'$ is executing $m'$, $t\neq t'$ and there is a method $\hat{m}$ with $m=\aliloctransx \hat{m}$ and $m'=\aliremtransx \hat{m}$.
\end{itemize}
}

\COMMENT{
%Intuitively, a well-formed run will have both the local and remote copy of a method (never only one of them)
The transformation as defined is both sound and complete which is given as the main result of this section.


\begin{theorem}
Let $P$ be a labelled program.
The TSO runs of $P$ and the SC runs of $\alisplitprogx P$ are isomorphic up to the rearrangement of the initialization of the remote methods, $M^{rem}_i$.
\end{theorem}
\begin{proof}[Sketch]
Let $\alisequencex r$ be a TSO run of $P$.
Then a run of $\alisplitprogx P$ is constructed by replacing all {\sc\small Init} transitions done by $t$ for method $m$ with an {\sc\small Init} transition done by $t^{loc}$ for $m^{loc}$ immediately followed by another {\sc\small Init} transition done by $t^{rem}$ for $m^{rem}$.
All local write transitions {\sc\small WrB} by $t$ are replaced with {\sc\small Wr} transitions by $t^{loc}$.
Similarly all buffered write transitions {\sc\small WrM} by $t$ are replaced with {\sc\small Wr} transitions by $t^{rem}$.
Each fence transition due to statement $s$ of $m$ by $t$ is replaced with $\aliremtransx s$ of $m^{rem}$ by $t^{rem}$.
By an inductive argument, in such cases the predicate in the assume instruction of $\aliremtransx s$ evaluates to true.

The construction in the other direction first moves all initialization transitions {\sc\small Init} of remote copies, i.e. $m^{rem}$ by some $t^{rem}$ immediately after its associated {\sc\small Init} transition of $m^{loc}$.
The replacement of the initialization of 
\end{proof}
}

%We now present a mapping which will output another program $P'$ such that there exists a bijection between $\alirunsx {tso} P$ and $\alirunsx {sc} P$. 



\COMMENT{
Observe that according to Def.~\ref{def:movers}, in order to prove that a statement $s$ is a left-mover we only need to check whether $s$ moves to the left of those statements with which $s$ can simultaneously execute. 
Let us consider a particular instance where $s^{loc}$ is a local write action executed by thread $t$, $s$ is its matching remote write action, and all occurrences of $s^{loc}$ are preceded by a fence statement executed by $t$.
This means that between $s^{loc}$ and $s$ there could be no remote write actions done by $t$.
This in turn implies that if $s$ is left-mover, then the combined write action is atomic.
}


 


\COMMENT{

\begin{definition}[Movers]
Let $\mathbf{E}$ be a set of TSO-executions, and $\aliequivgeneric$ be an equivalence relation over $\mathbf{E}$.
Let $a$ be some TSO action that occurs in some TSO-execution in $\mathbf{E}$.
Then, $a$ is called a {\em left mover per $\aliequivgeneric$} if for any TSO-execution $\mathbf{e}\in\mathbf{E}$ in which $a$ occurs, there is another TSO-execution $\mathbf{e}'\in\mathbf{E}$ such that $\mathbf{e}\aliequivgeneric\mathbf{e}'$ and each occurrence of $a$ in $\mathbf{e}'$ is either immediately preceded by actions that precede $a$ in $\alipotsox {\mathbf{e}'}$ or the first action in $\mathbf{e}'$.

Similarly, $a$ is called a {\em right mover per $\aliequivgeneric$} if for any TSO-execution $\mathbf{e}\in\mathbf{E}$ in which $a$ occurs, there is another TSO-execution $\mathbf{e}'\in\mathbf{E}$ such that $\mathbf{e}\aliequivgeneric\mathbf{e}'$ and each occurrence of $a$ in $\mathbf{e}'$ is either immediately followed by actions that succeed $a$ in $\alipotsox {\mathbf{e}'}$ or the last action in $\mathbf{e}'$.
\end{definition}

The preceding definition is more general than the classical definition of reduction which fixes the interpretation of $\aliequivgeneric$: two executions are equivalent if they have identical end-states.
The reason for the added flexibility should become clear when we introduce abstraction for programs.
However, we will drop the mention of the equivalence relation whenever it is irrelevant to the discussion or clear from the context.

Typically, one relies on an inductive argument to show that a particular action is a mover.
Let $\mathbf{e}$ be a TSO-execution in $\mathbf{E}$.
We say that $\mathbf{e}[i]$ moves to the left of $\mathbf{e}[i-1]$ if there is a TSO-execution $\mathbf{e}'$ such that $\mathbf{e}\aliequivgeneric\mathbf{e}'$, $\mathbf{e}\langle 1,i-2\rangle\cdot\mathbf{e}[i]=\mathbf{e}'\langle 1,i-1\rangle$ and $|\mathbf{e}|\geq|\mathbf{e}'|$.
Intuitively, we stay in the same equivalence class by moving $\mathbf{e}[i]$ one step to the left (towards the beginning of the sequence). 
Then, if one can show that a particular action $a$ moves to the left of all actions $b$ that can immediately precede $a$ except for those $c$ such that $c\alipotsox {\mathbf{e}} a$, then $a$ will be shown to be a left-mover in that equivalence class.
%These arguments will be readily used to argue the mover types of statements of a program.  

\begin{lemma}
Let $\mathbf{e}$ be a TSO-execution and let $A_r$ denote the set of actions $\{\mathbf{e}[i] \mid \mathbf{e}[i]\in\tsoalph_{\remwrite,-,-}\}$ and $A_l$ denote the set of actions $\{\mathbf{e}[i] \mid \mathbf{e}[i]\in\tsoalph_{\locwrite,-,-}\}$.
If each action in $A_r$ is a left-mover or each action in $A_l$ is a right-mover in $[\mathbf{e}]_{\tsoequiv}$, then $\mathbf{e}$ is SC-like.
\end{lemma}

Thus, it is sufficient to show that all remote write actions are left-movers or all local write actions are right-movers to conclude that a TSO-execution is SC-like.
It is possible to weaken the condition further as follows.

\begin{corollary}

\end{corollary}


}



\COMMENT{
Consider the code given in Fig.~\ref{fig:store-buffer}, which is derived from a classic example illustrating the main behavioral difference between TSO and SC architectures.
\begin{figure}[h]
\begin{tabular}{p{.14\textwidth}p{.14\textwidth}p{.14\textwidth}}
\begin{alltt}m1()
\{
 \(A\):=1;
 \(\alireg\)[1]:=\(B\);
\}\end{alltt} 
&
\begin{alltt}m2()
\{
 \(B\):=2;
\}\end{alltt}
&
\begin{alltt}m3()
\{
 \(\alireg\)[1]:=\(B\);
 \(\alireg\)[2]:=\(A\);
\}\end{alltt}\\
\multicolumn{3}{l}%
{$\locwritex t A 1 \cdot \alireadx t B 0 \cdot \locwritex u B 2 \cdot \remwritex u B 2$}\\
\multicolumn{3}{c}%
{$ \cdot\ \alireadx v B 2 \cdot \alireadx v A 0 \cdot \remwritex t A 1$}
\end{tabular}
\caption{Store buffering with non SC-like memory trace.}
\label{fig:store-buffer}
\end{figure}
The memory trace given in Fig.~\ref{fig:buffer-store} is TSO compliant.
It belongs to a run in which thread $t$ runs {\tt m1}, $u$ runs {\tt m2} and $v$ runs {\tt m3} exactly once.




In the remainder of this section, we work through an example program which contains a triangular race and hence is not initially amenable to an SC analysis.
We show how abstracting the program leads to a program which is SC-like.
}
